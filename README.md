# 智能聊天助手项目文档

## 项目概述

智能聊天助手是一个基于Streamlit开发的交互式聊天应用，集成了多种大语言模型，支持中英文双语对话。该应用提供了简洁美观的用户界面、实时对话交互，以及多种高级功能，包括智能模型选择、情感识别回应、领域专家模式和文档增强对话。

## 功能特点

### 基础功能
- **多语言支持**：支持中文和英文界面无缝切换
- **多模型集成**：支持千问(Qwen)、深度求索(DeepSeek)和Llama等多种大语言模型
- **会话管理**：创建和管理多个独立聊天会话，并自动保存历史记录
- **参数调整**：灵活调整模型参数，如温度和最大生成长度

### 高级功能
- **智能模型选择**：根据问题类型自动推荐并切换至最合适的模型
- **情感交互**：识别用户情绪并给予适当情感回应
- **领域专家模式**：提供教育、医疗、职场、理财、创业等多个专业领域的定制化回答
- **文档增强对话**：支持上传PDF、Word、图片等多种格式文档，基于文档内容回答问题
- **高级缓存**：智能缓存相似问题的回答，提高响应速度
- **错误恢复**：网络异常时自动重试并恢复会话

### 界面特性
- **响应式设计**：适应不同屏幕尺寸的界面布局
- **深色主题**：提供舒适的深色界面
- **多语言界面**：完整支持中英文切换，包括所有组件和上传界面
- **进度可视化**：文档处理时实时显示圆环进度指示器和处理状态
- **自动滚动**：新消息自动滚动到视图
- **平滑过渡**：各种操作均有平滑的过渡效果

## 技术架构

### 前端框架
- **Streamlit**：用于构建交互式Web应用的Python框架

### 后端服务
- **自定义API客户端**：与大语言模型API进行通信，支持重试机制和错误处理
- **会话状态管理**：使用Streamlit的会话状态功能管理用户会话
- **配置系统**：灵活的配置文件系统支持自定义各项参数

### 项目结构
```
智能聊天助手/
├── chatbot.py            # 主应用入口
├── config.json           # 全局配置文件
├── requirements.txt      # 依赖项列表
├── setup.bat             # 一键安装脚本
├── start.bat             # 一键启动脚本
├── setup.py              # 安装辅助脚本
├── __init__.py           # 包初始化文件
├── .streamlit/           # Streamlit配置目录
│   └── config.toml       # Streamlit配置文件
├── components/           # UI组件目录
│   ├── chat.py           # 聊天界面组件
│   └── sidebar.py        # 侧边栏组件
├── utils/                # 工具函数目录
│   ├── api.py            # API客户端
│   ├── config.py         # 配置加载工具
│   ├── domain_experts.py # 领域专家配置
│   ├── document_processor.py # 文档处理工具
│   ├── emotion_detector.py # 情感检测工具
│   ├── model_selector.py # 智能模型选择器
│   ├── setup_poppler.py  # Poppler安装助手(PDF处理)
│   └── theme.py          # 主题和样式定义
├── temp/                 # 临时文件目录
│   └── processed/        # 处理后的文档缓存
└── tools/                # 外部工具目录
    └── poppler-23.11.0/  # PDF处理工具
```

## 详细文件说明

### chatbot.py
主应用入口文件，负责初始化应用、管理会话状态和组织UI布局。处理顶层页面配置、语言切换、文档上传处理和用户输入处理。

### config.json
全局配置文件，包含API端点、缓存设置、UI选项、情感检测关键词、文档处理设置和多模态设置等。支持自定义修改以适应不同的API端点和模型配置。

### components/chat.py
聊天界面组件，负责显示聊天历史和处理用户输入。包含：
- 聊天消息渲染
- 消息发送和接收处理
- 情感分析集成
- 智能模型选择调用
- 自动滚动控制

### components/sidebar.py
侧边栏组件，提供会话管理、模型选择和参数设置功能：
- 会话列表管理
- 专家模式选择与切换
- 模型参数调整
- 智能功能开关控制

### utils/api.py
API客户端，负责与大语言模型API通信：
- 请求发送和响应处理
- 错误处理和重试机制
- 超时控制和会话管理
- 支持Ollama、OpenAI等多种API格式

### utils/emotion_detector.py
情感检测工具，分析用户文本中的情感倾向：
- 关键词匹配情感分析
- 情感回应生成
- 多语言支持

### utils/model_selector.py
智能模型选择器，根据问题内容推荐最合适的模型：
- 领域关键词分析
- 问题类型识别
- 历史对话分析
- 自适应模型选择算法

### utils/domain_experts.py
领域专家配置，为不同专业领域提供定制系统提示词：
- 教育、医疗、职场、理财、创业等领域专家配置
- 中英文双语专家系统提示词
- 专家描述和图标设置

### utils/config.py
配置加载工具，处理配置文件的读取和合并：
- 默认配置管理
- 用户配置加载
- 配置深度合并

### utils/theme.py
主题和样式定义，包含自定义CSS和JavaScript：
- 深色主题样式
- 动画和过渡效果
- 响应式布局调整
- 自定义组件样式

### utils/document_processor.py
文档处理工具，支持多种格式文档的文本提取：
- PDF文件处理与页面提取：
    - 使用 `pdf2image` 将PDF转换为图像，再利用OCR识别文字
    - 或使用 `PyPDF2` 直接提取文本（作为备选方案）
- Word文档内容解析：
    - 使用 `python-docx` 库解析 `.docx` 文件
    - 兼容 `.doc` 格式（可能需要额外处理）
- 图像文本识别：
    - 使用 OCR (光学字符识别) 技术从图像中提取文本
    - 支持多种图片格式，如 `jpg`, `jpeg`, `png`
- 文档缓存管理：
    - 缓存已处理的文档，提高响应速度
    - 可配置缓存大小和过期时间
- 多语言文档处理：
    - 支持多种语言的文档处理
    - 依赖于OCR引擎的多语言支持
- 文本提取后处理：
    - 清理提取的文本，例如移除多余的换行符和空格
    - 文本分块，以适应模型上下文窗口大小
- 错误处理：
    - 捕获并处理文档处理过程中可能出现的异常
    - 提供详细的错误信息，方便调试
- 进度指示：
    - 在处理文档时显示进度条，提供更好的用户体验
- 支持的格式：
    - PDF, Word (docx, doc), TXT, Markdown, CSV, 图片 (jpg, jpeg, png, webp)

### 批处理文件
- **setup.bat**: 一键安装所有依赖和工具，调用setup.py脚本
- **start.bat**: 一键启动应用，运行streamlit和chatbot.py

## 安装部署指南

### 系统要求
- Python 3.8 或更高版本
- pip 包管理器
- 至少 4GB 内存（推荐8GB+以获得更好性能）
- 网络连接（用于连接模型API）
- Ollama（推荐版本 0.1.23 或更高版本）

### Ollama 安装与配置
1. 下载并安装 Ollama：
   - Windows: https://ollama.com/download/windows
   - MacOS: https://ollama.com/download/mac
   - Linux: `curl -fsSL https://ollama.com/install.sh | sh`

2. 拉取必要的模型：
   ```bash
   ollama pull qwen2.5:3b      # 通义千问 3B 模型
   ollama pull deepseek-r1:8b  # 深度求索 8B 模型
   ollama pull llama3.1:latest # Llama 3.1 模型
   ollama pull granite3.2-vision:latest # 用于图像识别的模型
   ```

3. 确认模型已成功安装：
   ```bash
   ollama list
   ```
   
4. 启动 Ollama 服务：
   - Windows: Ollama 安装后会自动作为服务运行
   - MacOS/Linux: 在终端运行 `ollama serve`

5. 验证 Ollama API 是否正常工作：
   ```bash
   curl http://localhost:1314/api/tags
   ```
   应该返回已安装模型的 JSON 列表

### 依赖安装
1. 克隆或下载项目代码到本地目录
2. 打开命令行，进入项目根目录
3. 使用以下方法之一安装所需依赖：

   **方法1: 使用安装脚本(Windows)**
   ```bash
   setup.bat
   ```
   这将自动安装所有依赖和工具，包括PDF处理所需的Poppler

   **方法2: 直接使用pip**
   ```bash
   pip install -r requirements.txt
   python setup.py
   ```

   **方法3: 使用虚拟环境(推荐)**
   ```bash
   # 创建虚拟环境
   python -m venv venv
   
   # 激活虚拟环境
   # Windows:
   venv\Scripts\activate
   # MacOS/Linux:
   source venv/bin/activate
   
   # 安装依赖
   pip install -r requirements.txt
   python setup.py
   ```

### 配置设置
1. 编辑 `config.json` 文件，设置您的API端点：
   ```json
   {
     "api": {
       "endpoint": "http://localhost:1314/api/chat",
       "max_retries": 3,
       "retry_delay": 1,
       "timeout": 120
     }
   }
   ```

2. 自定义模型配置（如需要）：
   ```json
   "models": {
     "qwen2.5:3b": {
       "display_name": "Qwen 2.5-3B",
       "description": "通义千问2.5-3B模型，适合中文对话，轻量高效",
       "max_tokens": 4096,
       "context_window": 8192,
       "priority": 1
     },
     "deepseek-r1:8b": {
       "display_name": "DeepSeek 8B",
       "description": "深度求索8B模型，擅长中文理解和生成",
       "max_tokens": 4096,
       "context_window": 4096,
       "priority": 2
     },
     "llama3.1:latest": {
       "display_name": "Llama 3.1",
       "description": "Meta最新Llama 3.1模型，多语言能力强",
       "max_tokens": 4096,
       "context_window": 8192,
       "priority": 3
     }
   }
   ```

3. 调整情感检测关键词（如需要）：
   ```json
   "emotion_detection": {
     "enabled": true,
     "keywords": {
       "positive": ["开心", "高兴", "快乐", "满意", "感谢", "赞", "棒", "好"],
       "negative": ["失望", "生气", "伤心", "悲伤", "沮丧", "郁闷", "难过", "烦"],
       "neutral": ["可以", "还行", "一般", "凑合", "ok", "OK"]
     }
   }
   ```

   #### 重要：新增生成保护与滚动配置

   近期项目更新添加了几项与生成保护和自动滚动有关的配置，建议检查并根据需要调整：

   - `conversation.post_generate_cooldown_seconds`（浮点，默认 2.0）
      - 介绍：生成完成后前端建议等待的秒数，用于防止用户在模型还在收尾时重复提交。

   - `conversation.generating_watchdog_timeout`（浮点，默认 5.0）
      - 介绍：若 `is_generating` 被置为 true，但在该超时内没有新的流式数据或清理操作，系统将自动清理该状态以避免永久阻塞。

   - `ui.scroll_batching_delay_ms`（整数，默认 80）
      - 介绍：自动滚动的批处理延迟（毫秒），用于合并短时间内的多次 DOM 更改再执行滚动，避免频繁布局开销。

   - `ui.scroll_batch_threshold`（整数，默认 6）
      - 介绍：短时间内新增节点数超过该阈值时，触发立即快速滚动（非平滑），以保持在持续高频消息场景下的跟随性。

   详细说明请参阅：`docs/CONFIGURATION.md`


### 启动应用
直接启动**
```bash
streamlit run chatbot.py
```

应用将在浏览器中自动打开，默认地址为：http://localhost:8501

## 使用指南

### 基本操作

#### 开始对话
1. 在底部输入框中输入您的问题或消息
2. 按下回车键或点击发送按钮提交
3. 等待AI助手回复（过程中会显示"思考中..."）

#### 创建新对话
1. 点击侧边栏顶部的"➕ 新建聊天"按钮
2. 系统将创建一个新的对话会话
3. 您的第一条消息将自动成为该会话的标题

#### 切换会话
1. 在侧边栏的历史会话列表中找到要切换的会话
2. 点击该会话下方的"切换"按钮
3. 系统将立即显示该会话的历史记录

#### 切换语言
点击页面顶部的"中/EN"按钮在中英文界面间切换

### 高级功能

#### 专家模式
1. 在侧边栏的"专家模式"下拉菜单中选择需要的领域专家
2. 可选专家包括：教育专家、健康顾问、职业顾问、理财顾问、创业导师等
3. 切换后，AI助手将以该领域专家的身份回答问题
4. 专家描述将显示在下拉菜单下方，帮助您了解该专家的专长

#### 模型选择
1. 在"选择模型"下拉菜单中选择想要使用的AI模型
2. 可选模型包括：
   - qwen2.5:3b（通义千问，适合中文对话，轻量高效）
   - deepseek-r1:8b（深度求索，擅长中文理解和生成）
   - llama3.1:latest（Meta大模型，多语言能力强）
3. 每个模型的特性将显示在页面底部

#### 自动模型选择
1. 在"智能功能"下勾选"自动模型选择"选项
2. 系统将根据您提问的内容自动切换到最适合的模型
3. 模型切换时会显示提示信息

#### 情感响应
1. 在"智能功能"下勾选"情感响应"选项
2. 系统将分析您消息中的情感倾向
3. AI回复将根据检测到的情感进行相应调整，表现出共情能力

#### 文档增强对话
1. 在聊天界面下方的文档上传区域上传文档（支持PDF、Word、图片等格式）
2. 系统会自动处理文档并提取文本内容，显示处理进度环
3. 上传成功后，启用文档增强开关
4. 此时发送的问题将基于文档内容进行回答
5. 可以点击"预览"按钮查看提取的文档内容
6. 不需要文档增强时，可以关闭开关或点击"清除"按钮移除文档

## 进阶应用场景

### 教育辅导
选择"教育专家"模式，针对学习问题获取专业指导：
- 学习方法建议
- 知识点讲解
- 学习资源推荐

### 健康咨询
选择"健康顾问"模式，获取一般性健康知识：
- 健康生活方式建议
- 常见健康问题解答
- 健康饮食指导

### 职业发展
选择"职业顾问"模式，获取职场相关建议：
- 简历优化指导
- 面试技巧分享
- 职业规划建议

### 理财投资
选择"理财顾问"模式，获取个人理财建议：
- 基础理财知识
- 投资策略讨论
- 财务目标规划

### 创业指导
选择"创业导师"模式，获取创业相关建议：
- 创业计划书编写
- 市场分析方法
- 团队管理策略

### 文档问答
上传文档后使用文档增强功能，针对文档内容提问：
- 文档摘要生成
- 文档信息提取
- 专业文档解析
- 多语言文档处理

## 故障排除

### 常见问题

#### 无法连接到API
如果出现"HTTP错误 404: Not Found for url: http://localhost:1314/api/chat"错误：
1. 确保Ollama服务已启动
2. 验证Ollama监听端口是否为1314（默认）
3. 检查防火墙是否阻止了连接
4. 尝试重启Ollama服务

#### 模型加载失败
如果应用启动但模型无法加载：
1. 检查是否已安装所需模型(`ollama list`)
2. 尝试重新拉取模型(`ollama pull 模型名称`)
3. 检查磁盘空间是否足够
4. 验证模型名称是否与config.json中的配置一致

#### 应用崩溃
如果应用突然崩溃：
1. 检查控制台错误日志
2. 确保安装了所有依赖项
3. 验证Python版本是否兼容(3.8+)
4. 检查RAM是否足够(至少4GB)

#### 模型响应缓慢
如果模型响应非常慢：
1. 尝试使用更小的模型(如qwen2.5:3b)
2. 减小最大生成长度参数
3. 检查系统资源占用情况
4. 确保没有其他大型应用占用GPU或CPU资源

### 性能优化

1. **缓存配置**：调整config.json中的缓存设置可以提高响应速度
   ```json
   "cache": {
     "enabled": true,
     "ttl": 3600,
     "max_entries": 100
   }
   ```

2. **超时设置**：如果响应经常超时，可以增加超时时间
   ```json
   "api": {
     "timeout": 180
   }
   ```

3. **文档处理配置**：调整文档处理相关参数提高处理效率
   ```json
   "document_processing": {
     "max_pages": 10,
     "cache_size": 15,
     "show_in_main_ui": true,
     "supported_formats": ["pdf", "docx", "doc", "txt", "jpg", "jpeg", "png"]
   }
   ```

4. **模型优先级**：调整模型的优先级以优化自动选择算法
   ```json
   "models": {
     "qwen2.5:3b": {
       "priority": 1  // 优先级数字越小越优先使用
     }
   }
   ```

5. **设备资源**：
   - 如果电脑配置较低，减少同时运行的其他程序
   - 使用物理机而非虚拟机能获得更好性能
   - 显存充足的GPU可以显著提升模型加载和推理速度
   - SSD存储可以加快文档处理和模型加载速度

## 开发者指南

### 添加新模型
1. 在Ollama中拉取新模型：`ollama pull 新模型名称`
2. 在config.json的models部分添加新模型配置：
   ```json
   "新模型名称": {
     "display_name": "友好显示名称",
     "description": "模型描述",
     "max_tokens": 模型最大token数,
     "context_window": 上下文窗口大小,
     "priority": 优先级数值
   }
   ```
3. 更新sidebar.py中的可用模型列表

### 添加新的专家模式
1. 在utils/domain_experts.py中添加新专家定义
2. 为中英文界面分别添加系统提示词
3. 设置专家图标和描述

### 自定义主题
1. 修改utils/theme.py文件中的CSS定义
2. 调整颜色、字体和动画效果
3. 添加自定义组件样式

## 更新日志

### v1.3.0 (2025-04-15)
- 代码优化：清理项目中未使用的冗余代码和重复功能
- 合并批处理脚本：移除冗余的install.bat，简化安装流程
- 优化文档处理组件：提高各种格式文档的处理速度
- 改进用户界面：优化界面元素和动画效果
- 增强错误处理：添加更详细的错误提示
- 性能优化：减少内存占用和API调用次数

### v1.2.0 (2025-04-10)
- 新增文档处理功能，支持PDF、Word和图像识别
- 添加文档上传组件和文档增强对话功能
- 改进UI界面，添加进度显示和多语言支持
- 优化文档处理速度和准确性

### v1.1.0 (2025-04-6)
- 添加情感响应功能
- 改进智能模型选择算法
- 优化用户界面响应速度
- 多项BUG修复

### v1.0.0 (2025-04-1)
- 首次发布
- 基础聊天功能
- 多模型支持
- 中英文双语界面