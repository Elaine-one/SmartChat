# chat.py
import streamlit as st
import json
import time
from utils.config import CONFIG
from utils.model_selector import model_selector
from utils.emotion_detector import emotion_detector
from utils.document_processor import document_processor

# è·å–ç¼“å­˜é…ç½®
CACHE_CONFIG = CONFIG["cache"]

@st.cache_data(ttl=CACHE_CONFIG["ttl"], max_entries=CACHE_CONFIG["max_entries"])
def cached_generate_response(messages_json, model, temperature, max_tokens):
    """ç¼“å­˜æ¨¡å‹å“åº”ï¼Œé¿å…ç›¸åŒæé—®é‡å¤è¯·æ±‚API"""
    # è·å–LLMå®¢æˆ·ç«¯
    llm_client = st.session_state.llm_client
    # ä»JSONè¿˜åŸæ¶ˆæ¯
    messages = json.loads(messages_json)
    # è°ƒç”¨API
    return llm_client.generate_response(
        messages, 
        model,
        temperature=temperature,
        max_tokens=max_tokens
    )

def display_chat_history(messages, model_changes=None):
    """æ˜¾ç¤ºèŠå¤©å†å²"""
    # è·å–UIé…ç½®
    ui_config = CONFIG["ui"]
    max_messages = ui_config.get("max_message_display", 50)
    
    # è·å–å½“å‰è¯­è¨€
    is_chinese = st.session_state.get("language", "zh") == "zh"
    
    # å¦‚æœæ¶ˆæ¯è¿‡å¤šï¼Œåªæ˜¾ç¤ºæœ€è¿‘çš„max_messagesæ¡
    display_messages = messages[-max_messages:] if len(messages) > max_messages else messages
    
    # ç®€åŒ–çš„èŠå¤©å†å²æ˜¾ç¤ºï¼Œä½¿ç”¨è¡¨æƒ…ç¬¦å·ä½œä¸ºå¤´åƒ
    message_count = 0
    for msg in display_messages:
        # è·³è¿‡ç³»ç»Ÿæ¶ˆæ¯
        if msg["role"] == "system":
            continue
            
        # ç›´æ¥æ˜¾ç¤ºæ¶ˆæ¯å†…å®¹
        with st.chat_message(msg["role"], avatar="ğŸ‘¤" if msg["role"] == "user" else "ğŸ¤–"):
            st.markdown(msg["content"])
        message_count += 1
        
        # åœ¨æ¯æ¡åŠ©æ‰‹æ¶ˆæ¯åæ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³çš„æ¨¡å‹åˆ‡æ¢è®°å½•
        if msg["role"] == "assistant" and model_changes:
            # è®¡ç®—æ­¤æ¶ˆæ¯çš„ç´¢å¼•ä½ç½®
            current_index = message_count // 2  # æ¯å¯¹ç”¨æˆ·-åŠ©æ‰‹æ¶ˆæ¯ç®—ä¸€ç»„
            
            # ç­›é€‰å‡ºåº”è¯¥æ˜¾ç¤ºåœ¨è¿™æ¡æ¶ˆæ¯åçš„æ‰€æœ‰æ¨¡å‹åˆ‡æ¢è®°å½•
            relevant_changes = [change for change in model_changes 
                               if change.get("after_message_index", 0) == current_index]
            
            # å¦‚æœæœ‰ç›¸å…³çš„æ¨¡å‹åˆ‡æ¢è®°å½•ï¼Œæ˜¾ç¤ºå®ƒä»¬
            for change in relevant_changes:
                # è·å–å½“å‰è¯­è¨€
                is_chinese = st.session_state.get("language", "zh") == "zh"
                
                # æ˜¾ç¤ºè½»é‡çº§çš„æ¨¡å‹åˆ‡æ¢æç¤º
                model_change_text = f"æ¨¡å‹å·²ä» {change['from']} åˆ‡æ¢ä¸º {change['to']}" if is_chinese else f"Model changed from {change['from']} to {change['to']}"
                
                # å¦‚æœæ˜¯è‡ªåŠ¨åˆ‡æ¢çš„ï¼Œæ˜¾ç¤ºä¸åŒçš„æ–‡æœ¬
                if change.get("auto", False):
                    model_change_text = f"ç³»ç»Ÿè‡ªåŠ¨å°†æ¨¡å‹ä» {change['from']} åˆ‡æ¢ä¸º {change['to']}ï¼Œä»¥æ›´å¥½åœ°å›ç­”æ‚¨çš„é—®é¢˜" if is_chinese else f"System automatically changed model from {change['from']} to {change['to']} to better answer your question"
                
                st.markdown(
                    f"""<div style="text-align: center; padding: 5px; 
                    color: rgba(255,255,255,0.5); font-size: 0.8rem; 
                    margin: 10px 0; font-style: italic; border-top: 1px solid rgba(255,255,255,0.1);
                    border-bottom: 1px solid rgba(255,255,255,0.1); padding: 8px 0;">
                    {model_change_text}
                    </div>""", 
                    unsafe_allow_html=True
                )
    
    # ä¼˜åŒ–è‡ªåŠ¨æ»šåŠ¨ï¼šå®šä½èŠå¤©åŒºåŸŸçš„å¯æ»šåŠ¨çˆ¶å®¹å™¨ã€ä½¿ç”¨é˜²æŠ–/èŠ‚æµï¼Œå¹¶åœ¨ç”¨æˆ·ä¸»åŠ¨æ»šåŠ¨æ—¶æš‚åœè‡ªåŠ¨æ»šåŠ¨ï¼Œå‡å°‘å¡é¡¿
    if message_count > 0:
        st.markdown('<div id="chat-bottom"></div>', unsafe_allow_html=True)
        st.markdown(
            """
            <script>
            (function(){
                const CHAT_BOTTOM_ID = 'chat-bottom';
                let userInteracting = false;
                let interactionTimeout = null;
                let scrollContainer = null;
                let batchedScrollTimeout = null;
                const BATCHING_DELAY = 80; // ms

                function findScrollableAncestor(el){
                    let p = el.parentElement;
                    while(p){
                        const style = window.getComputedStyle(p);
                        const overflowY = style.overflowY;
                        if(overflowY === 'auto' || overflowY === 'scroll') return p;
                        p = p.parentElement;
                    }
                    return document.scrollingElement || document.documentElement;
                }

                function setUserInteracting(){
                    userInteracting = true;
                    if(interactionTimeout) clearTimeout(interactionTimeout);
                    interactionTimeout = setTimeout(()=>{ userInteracting = false; }, 1500);
                }

                // Fast immediate scroll to bottom (non-smooth) for high-frequency updates
                function immediateScroll(){
                    if(userInteracting) return;
                    const el = document.getElementById(CHAT_BOTTOM_ID);
                    if(!el) return;
                    try{
                        if(!scrollContainer) scrollContainer = findScrollableAncestor(el);
                        // Wait for next frame so layout/scrollHeight is updated
                        requestAnimationFrame(()=>{
                            try{
                                // If the scroll container is the document, use window.scrollTo as a reliable fallback
                                if(scrollContainer === document.scrollingElement || scrollContainer === document.documentElement){
                                    const h = Math.max(document.documentElement.scrollHeight || 0, document.body.scrollHeight || 0);
                                    window.scrollTo({ top: h, left: 0, behavior: 'auto' });
                                } else {
                                    scrollContainer.scrollTop = scrollContainer.scrollHeight;
                                }
                            }catch(e){
                                try{ el.scrollIntoView(false); }catch(e){}
                            }
                        });

                        // Fallback: schedule another immediate set after a short delay in case layout updates late
                        setTimeout(()=>{
                            try{
                                if(scrollContainer === document.scrollingElement || scrollContainer === document.documentElement){
                                    const h = Math.max(document.documentElement.scrollHeight || 0, document.body.scrollHeight || 0);
                                    window.scrollTo(0, h);
                                } else {
                                    scrollContainer.scrollTop = scrollContainer.scrollHeight;
                                }
                            }catch(e){/* ignore */}
                        }, 60);
                    }catch(e){
                        try{ el.scrollIntoView(false); }catch(e){}
                    }
                }

                // Batched scroll to avoid many layout/reflow calls during rapid appends
                function scheduleBatchedScroll(){
                    if(batchedScrollTimeout) return;
                    batchedScrollTimeout = setTimeout(()=>{
                        batchedScrollTimeout = null;
                        immediateScroll();
                    }, BATCHING_DELAY);
                }

                // Attach lightweight listeners to detect user intent to scroll
                document.addEventListener('wheel', setUserInteracting, {passive:true});
                document.addEventListener('touchstart', setUserInteracting, {passive:true});
                document.addEventListener('keydown', (e)=>{
                    if(['PageUp','PageDown','ArrowUp','ArrowDown','Home','End'].includes(e.key)) setUserInteracting();
                }, {passive:true});

                requestAnimationFrame(()=>{
                    const el = document.getElementById(CHAT_BOTTOM_ID);
                    if(!el) return;
                    scrollContainer = findScrollableAncestor(el);

                    try{
                        scrollContainer.addEventListener('scroll', setUserInteracting, {passive:true});
                    }catch(e){ /* ignore */ }

                    if(!window._chatObserverAdded){
                        const observer = new MutationObserver((mutations)=>{
                            // Count added nodes to decide immediate vs batched scroll
                            let added = 0;
                            for(const m of mutations){
                                if(m.addedNodes) added += m.addedNodes.length;
                                // small optimization: if any added node is our chat-bottom, prefer immediate
                                for(const node of m.addedNodes || []){
                                    if(node && node.querySelector && node.querySelector('#' + CHAT_BOTTOM_ID)){
                                        added += 1;
                                    }
                                }
                            }
                            if(added >= 6){
                                // many nodes added quickly â€” do an immediate fast scroll
                                immediateScroll();
                            } else {
                                // otherwise batch a single fast scroll shortly
                                scheduleBatchedScroll();
                            }
                        });
                        observer.observe(document.body, { childList: true, subtree: true });
                        window._chatObserverAdded = true;
                    }

                    // initial fast scroll when not interacting
                    immediateScroll();
                });
            })();
            </script>
            """,
            unsafe_allow_html=True
        )

def handle_user_input(prompt, messages, llm_client, settings):
    """å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶è·å–AIå›å¤"""
    # å¹¶å‘ä¿æŠ¤ï¼šå¦‚æœå½“å‰å·²æœ‰ç”Ÿæˆè¿›è¡Œä¸­ï¼Œå…ˆæ£€æŸ¥ watchdog ä»¥é¿å…æŒ‚èµ·çŠ¶æ€é•¿æœŸé˜»å¡
    now = time.time()
    if st.session_state.get('is_generating', False):
        # watchdog æ—¶é—´é˜ˆå€¼ï¼ˆç§’ï¼‰ï¼Œè¶…æ—¶åè®¤ä¸ºä¹‹å‰çš„ç”Ÿæˆå·²å¡ä½å¹¶æ¸…ç†
        watchdog_timeout = float(CONFIG.get('conversation', {}).get('generating_watchdog_timeout', 30.0))
        wd_ts = st.session_state.get('_generating_watchdog_ts', None)
        if wd_ts and (now - wd_ts) > watchdog_timeout:
            # æ¸…ç†æŒ‚èµ·çŠ¶æ€ï¼Œè®©æ–°çš„è¯·æ±‚å¯ä»¥ç»§ç»­
            try:
                st.session_state['is_generating'] = False
                if '_generating_watchdog_ts' in st.session_state:
                    del st.session_state['_generating_watchdog_ts']
            except Exception:
                pass
        else:
            is_chinese = st.session_state.get('language', 'zh') == 'zh'
            warning_text = "æ­£åœ¨ç”Ÿæˆï¼Œè¯·ç¨å€™å†è¯•..." if is_chinese else "AI is still responding, please wait..."
            st.warning(warning_text)
            return

    # æ ‡è®°æ­£åœ¨ç”Ÿæˆï¼Œç¦ç”¨è¾“å…¥ï¼ˆç”±ä¸»ç•Œé¢è¯»å–è¯¥çŠ¶æ€ï¼‰
    st.session_state['is_generating'] = True
    # watchdog æ—¶é—´æˆ³ï¼Œç”¨äºæ£€æµ‹å¡ä½çš„ç”Ÿæˆå¹¶åœ¨å¤–å±‚æ¢å¤
    import time as _time
    st.session_state['_generating_watchdog_ts'] = _time.time()

    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user", avatar="ğŸ‘¤"):
        st.markdown(prompt)
    
    # æ·»åŠ åˆ°æ¶ˆæ¯å†å²
    messages.append({"role": "user", "content": prompt})

    # è·å–ç³»ç»Ÿæç¤ºè¯
    system_prompt = settings["system_prompt"]
    
    # è·å–å½“å‰URLå‚æ•°ä¸­çš„è¯­è¨€è®¾ç½®ï¼Œè¿™æ›´èƒ½åæ˜ ç”¨æˆ·çš„å®é™…ç•Œé¢è¯­è¨€é€‰æ‹©
    query_params = st.query_params
    url_lang = query_params.get("lang", "zh")
    
    # ç¡®ä¿ç³»ç»Ÿæç¤ºè¯ä¸å½“å‰è¯­è¨€ä¸€è‡´
    is_chinese = url_lang == "zh"
    if is_chinese and "ä½ æ˜¯ä¸€ä¸ªå‹å¥½" not in system_prompt[:50]:
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå‹å¥½ã€ä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚ç”¨è‡ªç„¶çš„å£è¯­åŒ–é£æ ¼äº¤æµï¼Œå›ç­”ç®€æ´æ¸…æ™°ï¼ˆ1-3å¥è¯ï¼‰ã€‚éµå¾ªè§„åˆ™ï¼š  
1. ç§¯æå…±æƒ…ï¼Œé¿å…è´Ÿé¢è¡¨è¾¾  
2. ä¸æ¸…æ¥šæ—¶ç¤¼è²Œè¯¢é—®ç»†èŠ‚  
3. æ‹’ç»å›ç­”æ•æ„Ÿè¯é¢˜ï¼Œå¼•å¯¼è‡³å®‰å…¨æ–¹å‘  
4. é€‚å½“ä½¿ç”¨è¡¨æƒ…ç¬¦å·ï¼ˆå¦‚ğŸ˜Šï¼‰å¢åŠ äº²å’ŒåŠ›"""
    elif not is_chinese and "You are a friendly" not in system_prompt[:50]:
        system_prompt = """You are a friendly and helpful AI assistant. Use a natural, conversational style and keep answers concise (1-3 sentences). Follow these rules:
1. Be positive and empathetic, avoid negative expressions
2. Politely ask for clarification when unsure
3. Decline to answer sensitive topics and redirect to safe areas
4. Use appropriate emojis (like ğŸ˜Š) to add warmth"""
    
    # æƒ…æ„Ÿåˆ†æ - ä½¿ç”¨è®¾ç½®ä¸­çš„å¼€å…³
    emotion = None
    if settings.get("emotion_detection", False):
        emotion = emotion_detector.detect_emotion(prompt)
    
    # å¦‚æœæ£€æµ‹åˆ°æƒ…æ„Ÿï¼Œå¢å¼ºç³»ç»Ÿæç¤ºè¯
    emotional_response = ""
    if emotion:
        emotional_response = emotion_detector.get_emotional_response(emotion)
        if emotional_response and len(emotional_response) > 0:
            # å°†æƒ…æ„Ÿå›åº”æ·»åŠ åˆ°ç³»ç»Ÿæç¤ºè¯
            if is_chinese:
                system_prompt = system_prompt + f"\n\nç”¨æˆ·æƒ…ç»ªä¼¼ä¹æ˜¯{emotion}ã€‚åœ¨å›å¤çš„å¼€å¤´åŠ ä¸Šä»¥ä¸‹æƒ…æ„Ÿå›åº”ï¼š\n{emotional_response}"
            else:
                system_prompt = system_prompt + f"\n\nThe user's emotion seems to be {emotion}. Start your response with this emotional acknowledgement:\n{emotional_response}"
    
    # å¼ºåˆ¶å‘é€å†·å´ï¼šé˜²æ­¢ç”¨æˆ·è¿‡å¿«è¿ç»­æäº¤ï¼ˆå¦‚æœé…ç½®äº†å†·å´ï¼‰
    cooldown = float(settings.get("cooldown_seconds", CONFIG.get("conversation", {}).get("cooldown_seconds", 1.0)))
    now = time.time()
    last_send = st.session_state.get("_last_send_ts", 0)
    if cooldown > 0 and now - last_send < cooldown:
        remaining = round(cooldown - (now - last_send), 2)
        is_chinese = st.session_state.get('language', 'zh') == 'zh'
        st.warning((f"è¯·ç­‰å¾… {remaining} ç§’åå†å‘é€..." if is_chinese else f"Please wait {remaining}s before sending another message..."))
        st.session_state['is_generating'] = False
        return
    # æ›´æ–°æœ€åå‘é€æ—¶é—´æˆ³
    st.session_state["_last_send_ts"] = now

    # æ„é€ è¯·æ±‚æ•°æ® - ä½¿ç”¨ä¾§è¾¹æ é…ç½®çš„ä¸Šä¸‹æ–‡é•¿åº¦
    max_history = int(settings.get("max_history_messages", CONFIG.get("conversation", {}).get("max_history_messages", 10)))
    api_messages = [{"role": "system", "content": system_prompt}]
    api_messages += messages[-max_history:]
    
    # å¦‚æœå¼€å¯äº†é»˜è®¤ç®€æ´å›ç­”é€‰é¡¹å¹¶ä¸”æœªè‡ªå®šä¹‰ç³»ç»Ÿæç¤ºï¼Œåˆ™åœ¨system_promptä¸­åŠ å…¥ç®€æ´çº¦æŸ
    concise_default = bool(settings.get("concise_by_default", CONFIG.get("conversation", {}).get("concise_by_default", True)))
    if concise_default and "ç®€æ´" not in system_prompt and "concise" not in system_prompt[:50]:
        if is_chinese:
            system_prompt = system_prompt + "\n\nè¯·å°½é‡å›ç­”ç®€æ´ï¼ˆ1-3å¥ï¼‰ï¼Œå¿…è¦æ—¶ç»™å‡ºè¦ç‚¹ã€‚"
        else:
            system_prompt = system_prompt + "\n\nPlease keep answers concise (1-3 sentences) and provide key points when necessary."

    # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªåŠ¨æ¨¡å‹é€‰æ‹©
    if settings.get("auto_model_select", False):
        # ä½¿ç”¨æ™ºèƒ½æ¨¡å‹é€‰æ‹©å™¨ç¡®å®šæœ€ä½³æ¨¡å‹
        recommended_model = model_selector.select_model(prompt, messages)
        
        # å¦‚æœæ¨èæ¨¡å‹ä¸å½“å‰ä¸åŒï¼Œè¿›è¡Œæ¨¡å‹åˆ‡æ¢
        if recommended_model != settings["model"]:
            # è®°å½•æ¨¡å‹å˜æ›´
            if "model_changes" not in st.session_state.chat_histories[st.session_state.current_chat_id]:
                st.session_state.chat_histories[st.session_state.current_chat_id]["model_changes"] = []
            
            # è®¡ç®—å½“å‰æ¶ˆæ¯ç´¢å¼•
            assistant_messages = [i for i, msg in enumerate(messages) 
                                 if msg["role"] == "assistant"]
            last_assistant_index = len(assistant_messages) if assistant_messages else 0
            
            # æ·»åŠ æ¨¡å‹å˜æ›´è®°å½•
            st.session_state.chat_histories[st.session_state.current_chat_id]["model_changes"].append({
                "from": settings["model"],
                "to": recommended_model,
                "after_message_index": last_assistant_index,
                "displayed": False,
                "auto": True
            })
            
            # å°†æ¨¡å‹æ›´æ–°ä¸ºæ¨èæ¨¡å‹
            settings["model"] = recommended_model
            
            # æ›´æ–°ä¼šè¯çŠ¶æ€ä¸­çš„æ¨¡å‹
            st.session_state.model_choice = recommended_model
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†æ–‡æ¡£å¢å¼ºå›å¤åŠŸèƒ½
    document_enabled = st.session_state.get("document_enabled", False)
    document_text = st.session_state.get("document_text", "")
    
    # å¦‚æœå¯ç”¨äº†æ–‡æ¡£å¢å¼ºä¸”æœ‰æ–‡æ¡£å†…å®¹
    if document_enabled and document_text:
        try:
            with st.chat_message("assistant", avatar="ğŸ¤–"):
                # åˆ›å»ºæ–‡æ¡£å¢å¼ºæç¤º
                doc_info = (
                    "æˆ‘æ­£åœ¨ä½¿ç”¨æ–‡æ¡£å†…å®¹æ¥å›ç­”æ‚¨çš„é—®é¢˜..." if is_chinese else 
                    "I'm using the document content to answer your question..."
                )

                with st.spinner(doc_info):
                    # ä½¿ç”¨æ–‡æ¡£å¤„ç†å™¨ç”ŸæˆåŸºäºæ–‡æ¡£çš„å›å¤ï¼ˆåŒæ­¥ï¼‰
                    reply = document_processor.generate_document_enhanced_response(
                        prompt, 
                        document_text, 
                        settings["model"]
                    )

                    # æ˜¾ç¤ºå›å¤
                    st.markdown(reply)

            # è®°å½•å“åº”åˆ°èŠå¤©å†å²
            if reply:
                messages.append({"role": "assistant", "content": reply})
                # è®¾ç½®ç”Ÿæˆåå†·å´æ—¶é—´ï¼Œå‰ç«¯ä¼šä¾æ®è¯¥æ—¶é—´æ˜¾ç¤ºå‰©ä½™ç­‰å¾…
                try:
                    post_cd = float(settings.get("post_generate_cooldown_seconds", CONFIG.get("conversation", {}).get("post_generate_cooldown_seconds", 2.0)))
                    st.session_state['post_generate_cooldown_until'] = time.time() + post_cd
                except Exception:
                    pass
        finally:
            # ç”Ÿæˆç»“æŸï¼Œæ¢å¤è¾“å…¥
            st.session_state['is_generating'] = False

        # åœ¨æ–‡æ¡£å›å¤æ¨¡å¼ä¸‹ä¸éœ€è¦è°ƒç”¨æ™®é€šAPI
        return
    
    # ä½¿ç”¨æµå¼è¾“å‡ºAPI - ä»¥å¢é‡å½¢å¼æ¸²æŸ“å›å¤
    try:
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            placeholder = st.empty()
            accumulated = ""

            # é€‰æ‹©æµå¼ç”Ÿæˆå™¨æ¥å£ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            stream_generator = None
            # ä¼˜å…ˆä½¿ç”¨æ˜¾å¼çš„ generate_stream æ–¹æ³•ï¼ˆè¿”å›ç”Ÿæˆå™¨ï¼‰
            if hasattr(llm_client, 'generate_stream'):
                stream_generator = llm_client.generate_stream(
                    api_messages,
                    model=settings["model"],
                    temperature=settings["temperature"],
                    max_tokens=settings["max_tokens"]
                )
            else:
                # å¦åˆ™å°è¯•ä½¿ç”¨ generate_response è¿”å›çš„å¯è¿­ä»£å¯¹è±¡
                result = llm_client.generate_response(
                    api_messages,
                    settings["model"],
                    temperature=settings["temperature"],
                    max_tokens=settings["max_tokens"],
                    stream=True
                )
                # å¦‚æœè¿”å› (reply, error) çš„å½¢å¼ï¼Œå°è¯•å–ç¬¬ä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡
                if isinstance(result, tuple) and len(result) == 2:
                    stream_generator = result[0]
                else:
                    stream_generator = result

            # å¦‚æœstream_generatoræ˜¯å­—ç¬¦ä¸²ï¼ˆéè¿­ä»£å™¨ï¼‰ï¼Œç›´æ¥æ˜¾ç¤º
            if isinstance(stream_generator, str):
                accumulated = stream_generator
                placeholder.markdown(accumulated)
            else:
                # è¿­ä»£ç”Ÿæˆå—ï¼Œå¢é‡æ›´æ–°UI
                try:
                    for chunk in stream_generator:
                        # æœ‰äº›å®ç°å¯èƒ½yield Noneæˆ–ç©ºå­—ç¬¦ä¸²ï¼Œè·³è¿‡
                        if not chunk:
                            continue
                        accumulated += chunk
                        placeholder.markdown(accumulated)
                        # æ¯æ”¶åˆ°ä¸€æ¬¡ chunk åˆ·æ–° watchdog æ—¶é—´æˆ³ï¼Œè¡¨ç¤ºè¿˜åœ¨è¿›è¡Œä¸­
                        try:
                            st.session_state['_generating_watchdog_ts'] = _time.time()
                        except Exception:
                            pass
                except TypeError:
                    # éå¯è¿­ä»£è¿”å›ï¼Œå°è¯•ç›´æ¥æ˜¾ç¤ºå…¶å­—ç¬¦ä¸²è¡¨ç¤º
                    placeholder.markdown(str(stream_generator))

        # å°†å®Œæ•´å“åº”è®°å½•åˆ°å†å²ï¼ˆå¦‚æœæœ‰å†…å®¹ï¼‰
        if accumulated:
            messages.append({"role": "assistant", "content": accumulated})
            # è®°å½•ç”Ÿæˆåå†·å´ï¼Œå‰ç«¯ä¼šä¾æ®è¯¥æ—¶é—´æç¤ºç”¨æˆ·ç­‰å¾…
            try:
                post_cd = float(settings.get("post_generate_cooldown_seconds", CONFIG.get("conversation", {}).get("post_generate_cooldown_seconds", 2.0)))
                st.session_state['post_generate_cooldown_until'] = time.time() + post_cd
            except Exception:
                pass
        # ç¡®ä¿åœ¨æˆåŠŸå¤„ç†åæ¸…ç†ç”ŸæˆçŠ¶æ€ï¼Œé¿å…çŠ¶æ€æ®‹ç•™
        try:
            st.session_state['is_generating'] = False
        except Exception:
            pass

    except Exception as e:
        error_msg = f"æµå¼ç”Ÿæˆå‡ºé”™: {e}"
        st.error(error_msg)
        # å‡ºé”™æ—¶ç¡®ä¿æ¸…ç†ç”ŸæˆçŠ¶æ€
        try:
            st.session_state['is_generating'] = False
        except Exception:
            pass

    finally:
        # ç”Ÿæˆç»“æŸï¼Œæ¢å¤è¾“å…¥ï¼ˆå†åšä¸€æ¬¡ä¿é™©æ€§æ¸…ç†ï¼‰
        try:
            st.session_state['is_generating'] = False
        except Exception:
            pass
        # æ¸…ç† watchdog
        if '_generating_watchdog_ts' in st.session_state:
            try:
                del st.session_state['_generating_watchdog_ts']
            except Exception:
                pass